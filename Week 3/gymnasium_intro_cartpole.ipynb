{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to OpenAI Gymnasium with CartPole\n",
        "\n",
        "Before diving into policy gradient algorithms with MetaDrive, let's get familiar with OpenAI Gymnasium using a simpler environment: **CartPole**.\n",
        "\n",
        "## Goals\n",
        "* Understand the Gymnasium API\n",
        "* Connect MDP theory to a concrete environment\n",
        "* Practice implementing simple agents\n",
        "* Understand the environment-agent interaction loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is CartPole?\n",
        "\n",
        "CartPole is a classic reinforcement learning problem. A pole is attached to a cart that moves along a frictionless track. The goal is to keep the pole balanced upright by moving the cart left or right.\n",
        "\n",
        "Documentation: https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
        "\n",
        "# \n",
        "# ![CartPole Animation](https://gymnasium.farama.org/_images/cart_pole.gif)\n",
        "# \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/will/miniconda3/envs/torch/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/will/miniconda3/envs/torch/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/will/miniconda3/envs/torch/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/will/miniconda3/envs/torch/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/will/miniconda3/envs/torch/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/will/miniconda3/envs/torch/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install \"gymnasium[classic-control]\" numpy matplotlib ipython -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "from typing import Callable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CartPole as a Markov Decision Process (MDP)\n",
        "\n",
        "Recall from the [Policy Gradient notebook](policygradient.ipynb) that an MDP is defined as a tuple $(S, A, P, R, \\gamma)$:\n",
        "\n",
        "* **$S$**: The set of possible states\n",
        "* **$A$**: The set of possible actions\n",
        "* **$P$**: Transition probabilities $P_a(s, s') = \\Pr(s_{t+1} = s' | s_t = s, a_t = a)$\n",
        "* **$R$**: Reward function $R_a(s, s')$\n",
        "* **$\\gamma$**: Discount factor\n",
        "\n",
        "Let's see how CartPole maps to this framework:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation Space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
            "Action Space: Discrete(2)\n",
            "\n",
            "Number of actions: 2\n"
          ]
        }
      ],
      "source": [
        "# Create the CartPole environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "print(\"Observation Space:\", env.observation_space)\n",
        "print(\"Action Space:\", env.action_space)\n",
        "print(\"\")\n",
        "print(\"Number of actions:\", env.action_space.n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the MDP Components\n",
        "\n",
        "**State Space ($S$)**: A 4-dimensional continuous vector:\n",
        "1. Cart position\n",
        "2. Cart velocity\n",
        "3. Pole angle (in radians)\n",
        "4. Pole angular velocity\n",
        "\n",
        "**Action Space ($A$)**: Discrete with 2 choices:\n",
        "* `0`: Push cart to the left\n",
        "* `1`: Push cart to the right\n",
        "\n",
        "**Transition Function ($P$)**: The physics of the cart-pole system (handled by the environment)\n",
        "\n",
        "**Reward Function ($R$)**: +1 reward for every timestep the pole remains upright\n",
        "\n",
        "**Discount Factor ($\\gamma$)**: We'll set this ourselves when training agents (typically 0.95-0.99)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Environment-Agent Loop\n",
        "\n",
        "The fundamental interaction pattern in RL:\n",
        "\n",
        "```\n",
        "1. Environment provides initial observation\n",
        "2. Agent selects an action based on observation\n",
        "3. Environment executes the action\n",
        "4. Environment returns: next observation, reward, done flags\n",
        "5. Repeat until episode ends\n",
        "```\n",
        "\n",
        "Let's see this in action:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial observation: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ]\n",
            "\n",
            "After taking action 1:\n",
            "  New observation: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
            "  Reward: 1.0\n",
            "  Terminated: False\n",
            "  Truncated: False\n"
          ]
        }
      ],
      "source": [
        "# Reset the environment to get initial observation\n",
        "observation, info = env.reset(seed=42)\n",
        "print(f\"Initial observation: {observation}\")\n",
        "print(\"\")\n",
        "\n",
        "# Take a single step\n",
        "action = 1  # Push right\n",
        "observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "print(f\"After taking action {action}:\")\n",
        "print(f\"  New observation: {observation}\")\n",
        "print(f\"  Reward: {reward}\")\n",
        "print(f\"  Terminated: {terminated}\")\n",
        "print(f\"  Truncated: {truncated}\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Gymnasium API Functions\n",
        "\n",
        "1. **`reset()`**: Resets environment to initial state, returns `(observation, info)`\n",
        "\n",
        "2. **`step(action)`**: Executes an action, returns `(observation, reward, terminated, truncated, info)`\n",
        "   - `terminated`: Episode ended due to failure conditions\n",
        "   - `truncated`: Episode ended due to time limit\n",
        "\n",
        "3. **`action_space`**: Describes valid actions\n",
        "\n",
        "4. **`observation_space`**: Describes the observation format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n",
        "\n",
        "(We can just run them) - they render and plot stats about the running trajectories, respectively. :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def render_episode(agent: Callable[[np.ndarray], int], agent_name: str = \"Agent\"):\n",
        "    \"\"\"\n",
        "    Render an episode as an animation in the notebook.\n",
        "    \"\"\"\n",
        "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "    \n",
        "    frames = []\n",
        "    observation, info = env.reset()\n",
        "    frames.append(env.render())\n",
        "    \n",
        "    total_reward = 0\n",
        "    for step in range(500):\n",
        "        action = agent(observation)\n",
        "        observation, reward, terminated, truncated, info = env.step(action)\n",
        "        frames.append(env.render())\n",
        "        total_reward += reward\n",
        "        \n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    \n",
        "    env.close()\n",
        "    \n",
        "    # Create animation\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    ax.set_title(f\"{agent_name} (Total Reward: {total_reward})\")\n",
        "    ax.axis('off')\n",
        "    \n",
        "    img = ax.imshow(frames[0])\n",
        "    \n",
        "    def animate(i):\n",
        "        img.set_array(frames[i])\n",
        "        return [img]\n",
        "    \n",
        "    anim = animation.FuncAnimation(fig, animate, frames=len(frames), interval=50, blit=True)\n",
        "    plt.close()\n",
        "    \n",
        "    return HTML(anim.to_jshtml())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also plot statistics about the episode:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_episode_stats(agent: Callable, agent_name: str):\n",
        "    \"\"\"\n",
        "    Plot the pole angle and cart position over an episode.\n",
        "    \"\"\"\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    observations, actions, rewards, total_reward = run_episode(env, agent)\n",
        "    env.close()\n",
        "    \n",
        "    observations = np.array(observations)\n",
        "    cart_positions = observations[:, 0]\n",
        "    pole_angles = observations[:, 2]\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6))\n",
        "    \n",
        "    ax1.plot(cart_positions, linewidth=2)\n",
        "    ax1.set_ylabel('Cart Position', fontsize=11)\n",
        "    ax1.set_title(f'{agent_name} - Episode Performance (Total Reward: {total_reward})', fontsize=13)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    ax2.plot(pole_angles, linewidth=2, color='orange')\n",
        "    ax2.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
        "    ax2.set_xlabel('Timestep', fontsize=11)\n",
        "    ax2.set_ylabel('Pole Angle (radians)', fontsize=11)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running Episodes and Collecting Trajectories\n",
        "\n",
        "Here's a function to run a complete episode and collect the trajectory:\n",
        "\n",
        "As an exercise, replace the '???' with comments denoting what each step does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_episode(env: gym.Env, agent: Callable[[np.ndarray], int], max_steps: int = 500) -> tuple[list, list, list, float]:\n",
        "    \"\"\"\n",
        "    Run a single episode using the given agent.\n",
        "    \n",
        "    Args:\n",
        "        env: Gymnasium environment\n",
        "        agent: Function that takes observation and returns action\n",
        "        max_steps: Maximum steps per episode\n",
        "        \n",
        "    Returns:\n",
        "        observations: List of observations\n",
        "        actions: List of actions taken\n",
        "        rewards: List of rewards received\n",
        "        total_reward: Sum of all rewards\n",
        "    \"\"\"\n",
        "    observations = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    \n",
        "    # ???\n",
        "    observation, info = env.reset()\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        observations.append(observation)\n",
        "        \n",
        "        # ???\n",
        "        action = agent(observation)\n",
        "        actions.append(action)\n",
        "        \n",
        "        # ???\n",
        "        observation, reward, terminated, truncated, info = env.step(action)\n",
        "        rewards.append(reward)\n",
        "        \n",
        "        # ???\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    \n",
        "    total_reward = sum(rewards)\n",
        "    return observations, actions, rewards, total_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercises: Simple Agents\n",
        "\n",
        "Now it's your turn! To validate that our environment works as a MDP, try a few simple agent strategies and see how they perform.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Random Agent\n",
        "\n",
        "Implement an agent that randomly selects actions from the action space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_agent(observation: np.ndarray) -> int:\n",
        "    \"\"\"\n",
        "    Agent that randomly samples from the action space.\n",
        "    \n",
        "    Args:\n",
        "        observation: Current observation from environment (not used by this agent)\n",
        "        \n",
        "    Returns:\n",
        "        action: Random action (0 or 1)\n",
        "    \"\"\"\n",
        "    # TODO: Implement random action selection\n",
        "    # Hint: Understand the action space and generate a random value within it.\n",
        "    \n",
        "    raise NotImplementedError()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Render the random agent\n",
        "render_episode(random_agent, \"Random Agent\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot statistics\n",
        "plot_episode_stats(random_agent, \"Random Agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Constant Action Agent\n",
        "\n",
        "Implement an agent that always takes the same action (e.g., always pushes left)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def constant_agent(observation: np.ndarray) -> int:\n",
        "    \"\"\"\n",
        "    Agent that always takes the same action.\n",
        "    \n",
        "    Args:\n",
        "        observation: Current observation from environment\n",
        "        \n",
        "    Returns:\n",
        "        action: Always returns the same action (choose 0 or 1)\n",
        "    \"\"\"\n",
        "    # TODO: Implement constant action strategy\n",
        "    raise NotImplementedError()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Render the constant agent\n",
        "render_episode(constant_agent, \"Constant Agent\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot statistics\n",
        "plot_episode_stats(constant_agent, \"Constant Agent\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Design Your Own Heuristic Agent\n",
        "\n",
        "Now try to design a smarter agent that uses the observation to make decisions!\n",
        "\n",
        "**Observation components** (reminder):\n",
        "- `observation[0]`: Cart position\n",
        "- `observation[1]`: Cart velocity\n",
        "- `observation[2]`: Pole angle (in radians)\n",
        "- `observation[3]`: Pole angular velocity\n",
        "\n",
        "**Some ideas to explore**:\n",
        "- Push in the direction the pole is leaning\n",
        "- Consider the pole's angular velocity\n",
        "- Try to keep the cart centered\n",
        "- Use a combination of multiple observation features\n",
        "\n",
        "**Challenge**: Can you create a heuristic that achieves 200+ reward consistently?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def heuristic_agent(observation: np.ndarray) -> int:\n",
        "    \"\"\"\n",
        "    Design your own heuristic agent!\n",
        "    \n",
        "    Args:\n",
        "        observation: [cart_pos, cart_vel, pole_angle, pole_angular_vel]\n",
        "        \n",
        "    Returns:\n",
        "        action: 0 (left) or 1 (right)\n",
        "    \"\"\"\n",
        "    # TODO: Implement your heuristic strategy\n",
        "    # Experiment with different approaches!\n",
        "    raise NotImplementedError()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Render your heuristic agent\n",
        "render_episode(heuristic_agent, \"Heuristic Agent\")\n",
        "\n",
        "# May take some time - if your agent is good. : )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot statistics\n",
        "plot_episode_stats(heuristic_agent, \"Heuristic Agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pytorch Neural Networks\n",
        "\n",
        "An optional exercise if you're new to Pytorch.\n",
        "\n",
        "Before we can learn policies with neural networks, we need to understand how PyTorch enables us to compute gradients automatically.\n",
        "\n",
        "## Neural Networks as Parameterized Functions\n",
        "\n",
        "A neural network is simply a function $f_\\theta(x)$ where:\n",
        "- $x$ is the input (e.g., an observation)\n",
        "- $\\theta$ are the parameters (weights and biases)\n",
        "- $f_\\theta(x)$ is the output (e.g., action probabilities)\n",
        "\n",
        "The key insight: we can compute $\\nabla_\\theta f_\\theta(x)$ (the gradient with respect to parameters) automatically using **backpropagation**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining a Simple Network\n",
        "\n",
        "Let's create a simple feedforward network:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleNetwork(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
        "        super().__init__()\n",
        "        # Create two linear layers (nn.Linear) and store them as class attributes\n",
        "        self.fc1 = \n",
        "        self.fc2 =\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x =  # First layer + activation\n",
        "        x =         # Output layer\n",
        "        return x\n",
        "\n",
        "# Create a network: 4 inputs -> 16 hidden -> 2 outputs\n",
        "net = SimpleNetwork(input_size=4, hidden_size=16, output_size=2)\n",
        "print(net)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forward Pass: Computing $f_\\theta(x)$\n",
        "\n",
        "The forward pass computes the output for a given input:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dummy input (batch of 1, with 4 features)\n",
        "x = torch.tensor([[0.1, 0.2, 0.3, 0.4]])\n",
        "\n",
        "# Forward pass: compute f_θ(x)\n",
        "output = net(x)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Output: {output}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backward Pass: Computing $\\nabla_\\theta f_\\theta(x)$\n",
        "\n",
        "PyTorch's `.backward()` computes gradients automatically using the chain rule.\n",
        "\n",
        "Here's the key connection to math:\n",
        "- When we call `loss.backward()`, PyTorch computes $\\nabla_\\theta L$ where $L$ is our loss function\n",
        "- These gradients tell us how to adjust $\\theta$ to decrease the loss\n",
        "\n",
        "Note that the gradient with respect to $\\theta$ is always the same shape as $\\theta$; if $\\theta$ is a vector, $\\nabla_\\theta f$ is a vector of the same size, and if it's a tensor, $\\nabla_\\theta f$ is a tensor of the same shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple loss (e.g., mean of outputs)\n",
        "loss = output.mean()\n",
        "print(f\"Loss: {loss.item()}\")\n",
        "\n",
        "# Before backward: gradients are None\n",
        "print(f\"\\nGradient of fc1.weight before backward: {net.fc1.weight.grad}\")\n",
        "\n",
        "# Compute gradients: ∇_θ loss\n",
        "loss.backward()\n",
        "\n",
        "# After backward: gradients are computed!\n",
        "print(f\"Gradient of fc1.weight after backward:\")\n",
        "print(net.fc1.weight.grad)\n",
        "print(f\"Shape: {net.fc1.weight.grad.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizers: Using Gradients to Update Parameters\n",
        "\n",
        "Once we have gradients, we use an optimizer to update parameters:\n",
        "\n",
        "$$\\theta_{new} = \\theta_{old} - \\alpha \\nabla_\\theta L$$\n",
        "\n",
        "where $\\alpha$ is the learning rate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an optimizer\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop pattern:\n",
        "# 1. Zero out old gradients\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# 2. Forward pass\n",
        "output = net(x)\n",
        "loss = output.mean()\n",
        "\n",
        "# 3. Backward pass: compute ∇_θ loss\n",
        "loss.backward()\n",
        "\n",
        "# 4. Update parameters: θ_new = θ_old - α * ∇_θ loss\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Parameters updated!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider: \n",
        "\n",
        "1. Why do we *subtract* the gradient of the loss?\n",
        "2. What's $\\alpha$ used for?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Putting It All Together: Neural Network Agent\n",
        "\n",
        "Now let's combine everything: we'll create a policy network for CartPole and see how to train it!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Define a Policy Network\n",
        "\n",
        "For CartPole:\n",
        "- **Input**: 4-dimensional observation (cart position, cart velocity, pole angle, pole angular velocity)\n",
        "- **Output**: 2-dimensional action probabilities (left, right)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, obs_size, hidden_size, action_size):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(???, hidden_size) # What should the input and output weight shapes be?\n",
        "        self.fc2 = nn.Linear(hidden_size, ???)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "        \n",
        "        Args:\n",
        "            x: observation tensor of shape (batch_size, obs_size)\n",
        "            \n",
        "        Returns:\n",
        "            action_probs: probability distribution over actions\n",
        "        \"\"\"\n",
        "        x = F.relu(self.fc1(x))\n",
        "        logits = self.fc2(x)\n",
        "        # Convert logits to probabilities using softmax\n",
        "        action_probs = F.softmax(logits, dim=-1)\n",
        "        return action_probs\n",
        "\n",
        "# Create the policy network for CartPole\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "obs_size = env.observation_space.shape[0]  # 4\n",
        "action_size = env.action_space.n  # 2\n",
        "\n",
        "policy_net = PolicyNetwork(obs_size=obs_size, hidden_size=32, action_size=action_size)\n",
        "print(f\"Policy network created: {obs_size} -> 32 -> {action_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create an Agent Using the Network\n",
        "\n",
        "We'll create an agent that samples actions from the network's output distribution:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralAgent:\n",
        "    def __init__(self, policy_network):\n",
        "        self.policy_net = policy_network\n",
        "    \n",
        "    def __call__(self, observation: np.ndarray) -> int:\n",
        "        \"\"\"\n",
        "        Select an action based on the current observation.\n",
        "        \n",
        "        Args:\n",
        "            observation: numpy array of shape (obs_size,)\n",
        "            \n",
        "        Returns:\n",
        "            action: integer action (0 or 1 for CartPole)\n",
        "        \"\"\"\n",
        "        # Convert observation to tensor\n",
        "        obs_tensor = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "        \n",
        "        # Get action probabilities from network (no gradient needed for sampling)\n",
        "        with torch.no_grad():\n",
        "            action_probs = self.policy_net(obs_tensor)\n",
        "        \n",
        "        # Sample an action from the probability distribution\n",
        "        action = torch.multinomial(action_probs, num_samples=1).item()\n",
        "        \n",
        "        return action\n",
        "\n",
        "# Create an agent with our untrained network\n",
        "neural_agent = NeuralAgent(policy_net)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see how the untrained agent performs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the untrained agent\n",
        "obs, actions, rewards, total_reward = run_episode(env, neural_agent)\n",
        "print(f\"Untrained agent reward: {total_reward}\")\n",
        "\n",
        "# Render it\n",
        "render_episode(neural_agent, \"Untrained Neural Agent\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Collect Trajectories\n",
        "\n",
        "Before training, we need to collect experience from the environment:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect one episode\n",
        "observations, actions, rewards, total_reward = run_episode(env, neural_agent)\n",
        "print(f\"Collected {len(observations)} transitions\")\n",
        "print(f\"Total reward: {total_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Implement a Training Step (Exercise)\n",
        "\n",
        "Now it's your turn! We'll first implement a reward to go computation, which calculates the sum of all rewards from the present timestep until the trajectory's termination. For why we calculate this, refer to the derivation in `policygradient.ipynb` (week 2 reading)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_rewards_to_go(rewards: list[float], gamma: float = 0.99) -> list[float]:\n",
        "    \"\"\"\n",
        "    Compute the discounted reward-to-go for each timestep.\n",
        "    \n",
        "    Args:\n",
        "        rewards: list of rewards [r_0, r_1, ..., r_{T-1}]\n",
        "        gamma: discount factor\n",
        "        \n",
        "    Returns:\n",
        "        rewards_to_go: list where rewards_to_go[t] = sum_{t'=t}^{T-1} gamma^{t'-t} * r_{t'}\n",
        "    \"\"\"\n",
        "    # TODO: Implement reward-to-go computation\n",
        "    # Hint: Work backwards from the end of the episode\n",
        "    # rewards_to_go[T-1] = rewards[T-1]\n",
        "    # rewards_to_go[t] = rewards[t] + gamma * rewards_to_go[t+1]\n",
        "    \n",
        "    raise NotImplementedError()\n",
        "\n",
        "# Test with simple example\n",
        "# test_rewards = [1, 1, 1, 10]\n",
        "# rtg = compute_rewards_to_go(test_rewards, gamma=0.9)\n",
        "# print(f\"Rewards: {test_rewards}\")\n",
        "# print(f\"Rewards-to-go: {rtg}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Implement REINFORCE Training Step\n",
        "\n",
        "Now we can implement the REINFORCE policy gradient. :) The loss function is:\n",
        "\n",
        "$$L = -\\frac{1}{N}\\sum_{i=1}^{N} \\sum_{t=0}^{T_i-1} \\log \\pi_\\theta(a_{i,t} | s_{i,t}) \\hat{R}_{i,t}$$\n",
        "\n",
        "where:\n",
        "- $N$ is the number of trajectories (in our case, we flatten all transitions)\n",
        "- $\\pi_\\theta(a_t | s_t)$ is the probability the policy assigns to action $a_t$ given state $s_t$\n",
        "- $\\hat{R}_t$ is the reward-to-go from timestep $t$\n",
        "\n",
        "When we minimize this loss with gradient descent, we're doing gradient *ascent* on the expected return! Note that we're doing this kind of hacky trick because our optimizer expects a metric to minimize instead of maximize.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_step(policy_net, optimizer, observations, actions, rewards, gamma=0.99):\n",
        "    \"\"\"\n",
        "    Perform one REINFORCE training step on the policy network.\n",
        "    \n",
        "    Args:\n",
        "        policy_net: the policy network\n",
        "        optimizer: PyTorch optimizer\n",
        "        observations: list of observation arrays\n",
        "        actions: list of actions taken\n",
        "        rewards: list of rewards received\n",
        "        gamma: discount factor for reward-to-go\n",
        "        \n",
        "    Returns:\n",
        "        loss: the computed loss value\n",
        "    \"\"\"\n",
        "    # TODO: Implement the REINFORCE training step\n",
        "    # \n",
        "    # Steps:\n",
        "    # 1. Compute rewards-to-go using the function above\n",
        "    # 2. Convert observations to tensor: torch.tensor(np.array(observations), dtype=torch.float32)\n",
        "    # 3. Convert actions to tensor: torch.tensor(actions, dtype=torch.long)\n",
        "    # 4. Convert rewards_to_go to tensor: torch.tensor(rewards_to_go, dtype=torch.float32)\n",
        "    # 5. Get action probabilities from network: action_probs = policy_net(obs_tensor)\n",
        "    # 6. Get probabilities of actions actually taken:\n",
        "    #    taken_action_probs = action_probs.gather(1, action_tensor.unsqueeze(1)).squeeze()\n",
        "    # 7. Compute log probabilities: log_probs = torch.log(taken_action_probs + 1e-10)  # add small epsilon for numerical stability\n",
        "    # 8. Compute REINFORCE loss: loss = -(log_probs * rtg_tensor).mean()\n",
        "    # 9. Backprop: optimizer.zero_grad(), loss.backward(), optimizer.step()\n",
        "    # 10. Return loss.item()\n",
        "    \n",
        "    raise NotImplementedError()\n",
        "\n",
        "# Test your implementation (uncomment after implementing)\n",
        "# optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.01)\n",
        "# loss = train_step(policy_net, optimizer, observations, actions, rewards)\n",
        "# print(f\"Loss after training step: {loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Training Loop\n",
        "\n",
        "Once you've implemented `train_step`, try running a full training loop:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full training loop \n",
        "\n",
        "num_episodes = 200\n",
        "\n",
        "policy_net = PolicyNetwork(obs_size=obs_size, hidden_size=32, action_size=action_size)\n",
        "optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.01)\n",
        "neural_agent = NeuralAgent(policy_net)\n",
        "\n",
        "episode_rewards = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    # Collect one episode\n",
        "    observations, actions, rewards, total_reward = run_episode(env, neural_agent)\n",
        "    \n",
        "    # Train on this episode\n",
        "    loss = train_step(policy_net, optimizer, observations, actions, rewards)\n",
        "    \n",
        "    # Track progress\n",
        "    episode_rewards.append(total_reward)\n",
        "    \n",
        "    if episode % 20 == 0:\n",
        "        print(f\"Episode {episode}: Reward = {total_reward:.2f}, Loss = {loss:.4f}\")\n",
        "\n",
        "# Plot training progress\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(episode_rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Episode Reward')\n",
        "plt.title('Training Progress')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Render the trained agent\n",
        "render_episode(neural_agent, \"Trained Neural Agent\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Congratulations!** You've just implemented REINFORCE, one of the foundational policy gradient algorithms!\n",
        "\n",
        "This implementation includes the core concepts, but there are additional improvements you'll learn about later:\n",
        "- **Baselines**: Reducing variance in gradient estimates by subtracting a baseline from rewards\n",
        "- **Entropy bonuses**: Encouraging exploration by adding an entropy term to the loss\n",
        "- **Generalized Advantage Estimation (GAE)**: A more sophisticated way to estimate advantages\n",
        "\n",
        "These refinements can significantly improve training stability and performance!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
