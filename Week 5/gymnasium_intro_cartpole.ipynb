{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to OpenAI Gymnasium with CartPole\n",
        "\n",
        "Before diving into policy gradient algorithms, let's get familiar with OpenAI Gymnasium using a simple environment: **CartPole**.\n",
        "\n",
        "## Goals\n",
        "* Understand the Gymnasium API\n",
        "* Connect MDP theory to a concrete environment\n",
        "* Practice implementing simple agents\n",
        "* Understand the environment-agent interaction loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is CartPole?\n",
        "\n",
        "CartPole is a classic reinforcement learning problem. A pole is attached to a cart that moves along a frictionless track. The goal is to keep the pole balanced upright by moving the cart left or right.\n",
        "\n",
        "Documentation: https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
        "\n",
        "# \n",
        "# ![CartPole Animation](https://gymnasium.farama.org/_images/cart_pole.gif)\n",
        "# \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/will/miniconda3/envs/torch/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/will/miniconda3/envs/torch/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/will/miniconda3/envs/torch/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/will/miniconda3/envs/torch/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/will/miniconda3/envs/torch/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/will/miniconda3/envs/torch/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install \"gymnasium[classic-control]\" numpy matplotlib ipython -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "from typing import Callable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CartPole as a Markov Decision Process (MDP)\n",
        "\n",
        "Recall from the [Policy Gradient notebook](policygradient.ipynb) that an MDP is defined as a tuple $(S, A, P, R, \\gamma)$:\n",
        "\n",
        "* **$S$**: The set of possible states\n",
        "* **$A$**: The set of possible actions\n",
        "* **$P$**: Transition probabilities $P_a(s, s') = \\Pr(s_{t+1} = s' | s_t = s, a_t = a)$\n",
        "* **$R$**: Reward function $R_a(s, s')$\n",
        "* **$\\gamma$**: Discount factor\n",
        "\n",
        "Let's see how CartPole maps to this framework:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation Space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
            "Action Space: Discrete(2)\n",
            "\n",
            "Number of actions: 2\n"
          ]
        }
      ],
      "source": [
        "# Create the CartPole environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "print(\"Observation Space:\", env.observation_space)\n",
        "print(\"Action Space:\", env.action_space)\n",
        "print(\"\")\n",
        "print(\"Number of actions:\", env.action_space.n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the MDP Components\n",
        "\n",
        "**State Space ($S$)**: A 4-dimensional continuous vector:\n",
        "1. Cart position\n",
        "2. Cart velocity\n",
        "3. Pole angle (in radians)\n",
        "4. Pole angular velocity\n",
        "\n",
        "**Action Space ($A$)**: Discrete with 2 choices:\n",
        "* `0`: Push cart to the left\n",
        "* `1`: Push cart to the right\n",
        "\n",
        "**Transition Function ($P$)**: The physics of the cart-pole system (handled by the environment)\n",
        "\n",
        "**Reward Function ($R$)**: +1 reward for every timestep the pole remains upright\n",
        "\n",
        "**Discount Factor ($\\gamma$)**: We'll set this ourselves when training agents (typically 0.95-0.99)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Environment-Agent Loop\n",
        "\n",
        "The fundamental interaction pattern in RL:\n",
        "\n",
        "```\n",
        "1. Environment provides initial observation\n",
        "2. Agent selects an action based on observation\n",
        "3. Environment executes the action\n",
        "4. Environment returns: next observation, reward, done flags\n",
        "5. Repeat until episode ends\n",
        "```\n",
        "\n",
        "Let's see this in action:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial observation: [ 0.0273956  -0.00611216  0.03585979  0.0197368 ]\n",
            "\n",
            "After taking action 1:\n",
            "  New observation: [ 0.02727336  0.18847767  0.03625453 -0.26141977]\n",
            "  Reward: 1.0\n",
            "  Terminated: False\n",
            "  Truncated: False\n"
          ]
        }
      ],
      "source": [
        "# Reset the environment to get initial observation\n",
        "observation, info = env.reset(seed=42)\n",
        "print(f\"Initial observation: {observation}\")\n",
        "print(\"\")\n",
        "\n",
        "# Take a single step\n",
        "action = 1  # Push right\n",
        "observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "print(f\"After taking action {action}:\")\n",
        "print(f\"  New observation: {observation}\")\n",
        "print(f\"  Reward: {reward}\")\n",
        "print(f\"  Terminated: {terminated}\")\n",
        "print(f\"  Truncated: {truncated}\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Gymnasium API Functions\n",
        "\n",
        "1. **`reset()`**: Resets environment to initial state, returns `(observation, info)`\n",
        "\n",
        "2. **`step(action)`**: Executes an action, returns `(observation, reward, terminated, truncated, info)`\n",
        "   - `terminated`: Episode ended due to failure conditions\n",
        "   - `truncated`: Episode ended due to time limit\n",
        "\n",
        "3. **`action_space`**: Describes valid actions\n",
        "\n",
        "4. **`observation_space`**: Describes the observation format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n",
        "\n",
        "(We can just run them) - they render and plot stats about the running trajectories, respectively. :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def render_episode(agent: Callable[[np.ndarray], int], agent_name: str = \"Agent\"):\n",
        "    \"\"\"\n",
        "    Render an episode as an animation in the notebook.\n",
        "    \"\"\"\n",
        "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "    \n",
        "    frames = []\n",
        "    observation, info = env.reset()\n",
        "    frames.append(env.render())\n",
        "    \n",
        "    total_reward = 0\n",
        "    for step in range(500):\n",
        "        action = agent(observation)\n",
        "        observation, reward, terminated, truncated, info = env.step(action)\n",
        "        frames.append(env.render())\n",
        "        total_reward += reward\n",
        "        \n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    \n",
        "    env.close()\n",
        "    \n",
        "    # Create animation\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    ax.set_title(f\"{agent_name} (Total Reward: {total_reward})\")\n",
        "    ax.axis('off')\n",
        "    \n",
        "    img = ax.imshow(frames[0])\n",
        "    \n",
        "    def animate(i):\n",
        "        img.set_array(frames[i])\n",
        "        return [img]\n",
        "    \n",
        "    anim = animation.FuncAnimation(fig, animate, frames=len(frames), interval=50, blit=True)\n",
        "    plt.close()\n",
        "    \n",
        "    return HTML(anim.to_jshtml())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also plot statistics about the episode:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_episode_stats(agent: Callable, agent_name: str):\n",
        "    \"\"\"\n",
        "    Plot the pole angle and cart position over an episode.\n",
        "    \"\"\"\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    observations, actions, rewards, total_reward = run_episode(env, agent)\n",
        "    env.close()\n",
        "    \n",
        "    observations = np.array(observations)\n",
        "    cart_positions = observations[:, 0]\n",
        "    pole_angles = observations[:, 2]\n",
        "    \n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6))\n",
        "    \n",
        "    ax1.plot(cart_positions, linewidth=2)\n",
        "    ax1.set_ylabel('Cart Position', fontsize=11)\n",
        "    ax1.set_title(f'{agent_name} - Episode Performance (Total Reward: {total_reward})', fontsize=13)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    ax2.plot(pole_angles, linewidth=2, color='orange')\n",
        "    ax2.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
        "    ax2.set_xlabel('Timestep', fontsize=11)\n",
        "    ax2.set_ylabel('Pole Angle (radians)', fontsize=11)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running Episodes and Collecting Trajectories\n",
        "\n",
        "Here's a function to run a complete episode and collect the trajectory:\n",
        "\n",
        "As an exercise, replace the '???' with comments denoting what each step does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_episode(env: gym.Env, agent: Callable[[np.ndarray], int], max_steps: int = 500) -> tuple[list, list, list, float]:\n",
        "    \"\"\"\n",
        "    Run a single episode using the given agent.\n",
        "    \n",
        "    Args:\n",
        "        env: Gymnasium environment\n",
        "        agent: Function that takes observation and returns action\n",
        "        max_steps: Maximum steps per episode\n",
        "        \n",
        "    Returns:\n",
        "        observations: List of observations\n",
        "        actions: List of actions taken\n",
        "        rewards: List of rewards received\n",
        "        total_reward: Sum of all rewards\n",
        "    \"\"\"\n",
        "    observations = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    \n",
        "    # ???\n",
        "    observation, info = env.reset()\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        observations.append(observation)\n",
        "        \n",
        "        # ???\n",
        "        action = agent(observation)\n",
        "        actions.append(action)\n",
        "        \n",
        "        # ???\n",
        "        observation, reward, terminated, truncated, info = env.step(action)\n",
        "        rewards.append(reward)\n",
        "        \n",
        "        # ???\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    \n",
        "    total_reward = sum(rewards)\n",
        "    return observations, actions, rewards, total_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercises: Simple Agents\n",
        "\n",
        "Now it's your turn! To validate that our environment works as a MDP, try a few simple agent strategies and see how they perform.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Random Agent\n",
        "\n",
        "Implement an agent that randomly selects actions from the action space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_agent(observation: np.ndarray) -> int:\n",
        "    \"\"\"\n",
        "    Agent that randomly samples from the action space.\n",
        "    \n",
        "    Args:\n",
        "        observation: Current observation from environment (not used by this agent)\n",
        "        \n",
        "    Returns:\n",
        "        action: Random action (0 or 1)\n",
        "    \"\"\"\n",
        "    # TODO: Implement random action selection\n",
        "    # Hint: Understand the action space and generate a random value within it.\n",
        "    \n",
        "    raise NotImplementedError()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Render the random agent\n",
        "render_episode(random_agent, \"Random Agent\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot statistics\n",
        "plot_episode_stats(random_agent, \"Random Agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Constant Action Agent\n",
        "\n",
        "Implement an agent that always takes the same action (e.g., always pushes left)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def constant_agent(observation: np.ndarray) -> int:\n",
        "    \"\"\"\n",
        "    Agent that always takes the same action.\n",
        "    \n",
        "    Args:\n",
        "        observation: Current observation from environment\n",
        "        \n",
        "    Returns:\n",
        "        action: Always returns the same action (choose 0 or 1)\n",
        "    \"\"\"\n",
        "    # TODO: Implement constant action strategy\n",
        "    raise NotImplementedError()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Render the constant agent\n",
        "render_episode(constant_agent, \"Constant Agent\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot statistics\n",
        "plot_episode_stats(constant_agent, \"Constant Agent\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Design Your Own Heuristic Agent\n",
        "\n",
        "Now try to design a smarter agent that uses the observation to make decisions!\n",
        "\n",
        "**Observation components** (reminder):\n",
        "- `observation[0]`: Cart position\n",
        "- `observation[1]`: Cart velocity\n",
        "- `observation[2]`: Pole angle (in radians)\n",
        "- `observation[3]`: Pole angular velocity\n",
        "\n",
        "**Some ideas to explore**:\n",
        "- Push in the direction the pole is leaning\n",
        "- Consider the pole's angular velocity\n",
        "- Try to keep the cart centered\n",
        "- Use a combination of multiple observation features\n",
        "\n",
        "**Challenge**: Can you create a heuristic that achieves 200+ reward consistently?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def heuristic_agent(observation: np.ndarray) -> int:\n",
        "    \"\"\"\n",
        "    Design your own heuristic agent!\n",
        "    \n",
        "    Args:\n",
        "        observation: [cart_pos, cart_vel, pole_angle, pole_angular_vel]\n",
        "        \n",
        "    Returns:\n",
        "        action: 0 (left) or 1 (right)\n",
        "    \"\"\"\n",
        "    # TODO: Implement your heuristic strategy\n",
        "    # Experiment with different approaches!\n",
        "    raise NotImplementedError()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Render your heuristic agent\n",
        "render_episode(heuristic_agent, \"Heuristic Agent\")\n",
        "\n",
        "# May take some time - if your agent is good. : )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot statistics\n",
        "plot_episode_stats(heuristic_agent, \"Heuristic Agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pytorch Neural Networks\n",
        "\n",
        "An optional exercise if you're new to Pytorch.\n",
        "\n",
        "Before we can learn policies with neural networks, we need to understand how PyTorch enables us to compute gradients automatically.\n",
        "\n",
        "## Neural Networks as Parameterized Functions\n",
        "\n",
        "A neural network is simply a function $f_\\theta(x)$ where:\n",
        "- $x$ is the input (e.g., an observation)\n",
        "- $\\theta$ are the parameters (weights and biases)\n",
        "- $f_\\theta(x)$ is the output (e.g., action probabilities)\n",
        "\n",
        "The key insight: we can compute $\\nabla_\\theta f_\\theta(x)$ (the gradient with respect to parameters) automatically using **backpropagation**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining a Simple Network\n",
        "\n",
        "Let's create a simple feedforward network:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleNetwork(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
        "        super().__init__()\n",
        "        # Create two linear layers (nn.Linear) and store them as class attributes\n",
        "        self.fc1 = \n",
        "        self.fc2 =\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x =  # First layer + activation\n",
        "        x =         # Output layer\n",
        "        return x\n",
        "\n",
        "# Create a network: 4 inputs -> 16 hidden -> 2 outputs\n",
        "net = SimpleNetwork(input_size=4, hidden_size=16, output_size=2)\n",
        "print(net)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forward Pass: Computing $f_\\theta(x)$\n",
        "\n",
        "The forward pass computes the output for a given input:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dummy input (batch of 1, with 4 features)\n",
        "x = torch.tensor([[0.1, 0.2, 0.3, 0.4]])\n",
        "\n",
        "# Forward pass: compute f_θ(x)\n",
        "output = net(x)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Output: {output}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backward Pass: Computing $\\nabla_\\theta f_\\theta(x)$\n",
        "\n",
        "PyTorch's `.backward()` computes gradients automatically using the chain rule.\n",
        "\n",
        "Here's the key connection to math:\n",
        "- When we call `loss.backward()`, PyTorch computes $\\nabla_\\theta L$ where $L$ is our loss function\n",
        "- These gradients tell us how to adjust $\\theta$ to decrease the loss\n",
        "\n",
        "Note that the gradient with respect to $\\theta$ is always the same shape as $\\theta$; if $\\theta$ is a vector, $\\nabla_\\theta f$ is a vector of the same size, and if it's a tensor, $\\nabla_\\theta f$ is a tensor of the same shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple loss (e.g., mean of outputs)\n",
        "loss = output.mean()\n",
        "print(f\"Loss: {loss.item()}\")\n",
        "\n",
        "# Before backward: gradients are None\n",
        "print(f\"\\nGradient of fc1.weight before backward: {net.fc1.weight.grad}\")\n",
        "\n",
        "# Compute gradients: ∇_θ loss\n",
        "loss.backward()\n",
        "\n",
        "# After backward: gradients are computed!\n",
        "print(f\"Gradient of fc1.weight after backward:\")\n",
        "print(net.fc1.weight.grad)\n",
        "print(f\"Shape: {net.fc1.weight.grad.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizers: Using Gradients to Update Parameters\n",
        "\n",
        "Once we have gradients, we use an optimizer to update parameters:\n",
        "\n",
        "$$\\theta_{new} = \\theta_{old} - \\alpha \\nabla_\\theta L$$\n",
        "\n",
        "where $\\alpha$ is the learning rate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an optimizer\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop pattern:\n",
        "# 1. Zero out old gradients\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# 2. Forward pass\n",
        "output = net(x)\n",
        "loss = output.mean()\n",
        "\n",
        "# 3. Backward pass: compute ∇_θ loss\n",
        "loss.backward()\n",
        "\n",
        "# 4. Update parameters: θ_new = θ_old - α * ∇_θ loss\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Parameters updated!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider: \n",
        "\n",
        "1. Why do we *subtract* the gradient of the loss?\n",
        "2. What's $\\alpha$ used for?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Putting It All Together: Neural Network Agent\n",
        "\n",
        "Now let's combine everything: we'll create a policy network for CartPole and see how to train it!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Define a Policy Network\n",
        "\n",
        "For CartPole:\n",
        "- **Input**: 4-dimensional observation (cart position, cart velocity, pole angle, pole angular velocity)\n",
        "- **Output**: 2-dimensional action probabilities (left, right)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, obs_size, hidden_size, action_size):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(???, hidden_size) # What should the input and output weight shapes be?\n",
        "        self.fc2 = nn.Linear(hidden_size, ???)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "        \n",
        "        Args:\n",
        "            x: observation tensor of shape (batch_size, obs_size)\n",
        "            \n",
        "        Returns:\n",
        "            action_probs: probability distribution over actions\n",
        "        \"\"\"\n",
        "        x = F.relu(self.fc1(x))\n",
        "        logits = self.fc2(x)\n",
        "        # Convert logits to probabilities using softmax\n",
        "        action_probs = F.softmax(logits, dim=-1)\n",
        "        return action_probs\n",
        "\n",
        "# Create the policy network for CartPole\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "obs_size = env.observation_space.shape[0]  # 4\n",
        "action_size = env.action_space.n  # 2\n",
        "\n",
        "policy_net = PolicyNetwork(obs_size=obs_size, hidden_size=32, action_size=action_size)\n",
        "print(f\"Policy network created: {obs_size} -> 32 -> {action_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create an Agent Using the Network\n",
        "\n",
        "We'll create an agent that samples actions from the network's output distribution:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralAgent:\n",
        "    def __init__(self, policy_network):\n",
        "        self.policy_net = policy_network\n",
        "    \n",
        "    def __call__(self, observation: np.ndarray) -> int:\n",
        "        \"\"\"\n",
        "        Select an action based on the current observation.\n",
        "        \n",
        "        Args:\n",
        "            observation: numpy array of shape (obs_size,)\n",
        "            \n",
        "        Returns:\n",
        "            action: integer action (0 or 1 for CartPole)\n",
        "        \"\"\"\n",
        "        # Convert observation to tensor\n",
        "        obs_tensor = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
        "        \n",
        "        # Get action probabilities from network (no gradient needed for sampling)\n",
        "        with torch.no_grad():\n",
        "            action_probs = self.policy_net(obs_tensor)\n",
        "        \n",
        "        # Sample an action from the probability distribution\n",
        "        action = torch.multinomial(action_probs, num_samples=1).item()\n",
        "        \n",
        "        return action\n",
        "\n",
        "# Create an agent with our untrained network\n",
        "neural_agent = NeuralAgent(policy_net)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see how the untrained agent performs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the untrained agent\n",
        "obs, actions, rewards, total_reward = run_episode(env, neural_agent)\n",
        "print(f\"Untrained agent reward: {total_reward}\")\n",
        "\n",
        "# Render it\n",
        "render_episode(neural_agent, \"Untrained Neural Agent\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Collect Trajectories\n",
        "\n",
        "Before training, we need to collect experience from the environment:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect one episode\n",
        "observations, actions, rewards, total_reward = run_episode(env, neural_agent)\n",
        "print(f\"Collected {len(observations)} transitions\")\n",
        "print(f\"Total reward: {total_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Implement a Training Step (Exercise)\n",
        "\n",
        "Now it's your turn! We'll first implement a reward to go computation, which calculates the sum of all rewards from the present timestep until the trajectory's termination. For why we calculate this, refer to the derivation in `policygradient.ipynb` (week 2 reading)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_rewards_to_go(rewards: list[float], gamma: float = 0.99) -> list[float]:\n",
        "    \"\"\"\n",
        "    Compute the discounted reward-to-go for each timestep.\n",
        "    \n",
        "    Args:\n",
        "        rewards: list of rewards [r_0, r_1, ..., r_{T-1}]\n",
        "        gamma: discount factor\n",
        "        \n",
        "    Returns:\n",
        "        rewards_to_go: list where rewards_to_go[t] = sum_{t'=t}^{T-1} gamma^{t'-t} * r_{t'}\n",
        "    \"\"\"\n",
        "    # TODO: Implement reward-to-go computation\n",
        "    # Hint: Work backwards from the end of the episode\n",
        "    # rewards_to_go[T-1] = rewards[T-1]\n",
        "    # rewards_to_go[t] = rewards[t] + gamma * rewards_to_go[t+1]\n",
        "    \n",
        "    raise NotImplementedError()\n",
        "\n",
        "# Test with simple example\n",
        "# test_rewards = [1, 1, 1, 10]\n",
        "# rtg = compute_rewards_to_go(test_rewards, gamma=0.9)\n",
        "# print(f\"Rewards: {test_rewards}\")\n",
        "# print(f\"Rewards-to-go: {rtg}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Implement REINFORCE Training Step\n",
        "\n",
        "Now we can implement the REINFORCE policy gradient. :) The loss function is:\n",
        "\n",
        "$$L = -\\frac{1}{N}\\sum_{i=1}^{N} \\sum_{t=0}^{T_i-1} \\log \\pi_\\theta(a_{i,t} | s_{i,t}) \\hat{R}_{i,t}$$\n",
        "\n",
        "where:\n",
        "- $N$ is the number of trajectories (in our case, we flatten all transitions)\n",
        "- $\\pi_\\theta(a_t | s_t)$ is the probability the policy assigns to action $a_t$ given state $s_t$\n",
        "- $\\hat{R}_t$ is the reward-to-go from timestep $t$\n",
        "\n",
        "When we minimize this loss with gradient descent, we're doing gradient *ascent* on the expected return! Note that we're doing this kind of hacky trick because our optimizer expects a metric to minimize instead of maximize.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_step(policy_net, optimizer, observations, actions, rewards, gamma=0.99):\n",
        "    \"\"\"\n",
        "    Perform one REINFORCE training step on the policy network.\n",
        "    \n",
        "    Args:\n",
        "        policy_net: the policy network\n",
        "        optimizer: PyTorch optimizer\n",
        "        observations: list of observation arrays\n",
        "        actions: list of actions taken\n",
        "        rewards: list of rewards received\n",
        "        gamma: discount factor for reward-to-go\n",
        "        \n",
        "    Returns:\n",
        "        loss: the computed loss value\n",
        "    \"\"\"\n",
        "    # TODO: Implement the REINFORCE training step\n",
        "    # \n",
        "    # Steps:\n",
        "    # 1. Compute rewards-to-go using the function above\n",
        "    # 2. Convert observations to tensor: torch.tensor(np.array(observations), dtype=torch.float32)\n",
        "    # 3. Convert actions to tensor: torch.tensor(actions, dtype=torch.long)\n",
        "    # 4. Convert rewards_to_go to tensor: torch.tensor(rewards_to_go, dtype=torch.float32)\n",
        "    # 5. Get action probabilities from network: action_probs = policy_net(obs_tensor)\n",
        "    # 6. Get probabilities of actions actually taken:\n",
        "    #    taken_action_probs = action_probs.gather(1, action_tensor.unsqueeze(1)).squeeze()\n",
        "    # 7. Compute log probabilities: log_probs = torch.log(taken_action_probs + 1e-10)  # add small epsilon for numerical stability\n",
        "    # 8. Compute REINFORCE loss: loss = -(log_probs * rtg_tensor).mean()\n",
        "    # 9. Backprop: optimizer.zero_grad(), loss.backward(), optimizer.step()\n",
        "    # 10. Return loss.item()\n",
        "    \n",
        "    raise NotImplementedError()\n",
        "\n",
        "# Test your implementation (uncomment after implementing)\n",
        "# optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.01)\n",
        "# loss = train_step(policy_net, optimizer, observations, actions, rewards)\n",
        "# print(f\"Loss after training step: {loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Training Loop\n",
        "\n",
        "Once you've implemented `train_step`, try running a full training loop:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full training loop \n",
        "\n",
        "num_episodes = 200\n",
        "\n",
        "policy_net = PolicyNetwork(obs_size=obs_size, hidden_size=32, action_size=action_size)\n",
        "optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.01)\n",
        "neural_agent = NeuralAgent(policy_net)\n",
        "\n",
        "episode_rewards = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    # Collect one episode\n",
        "    observations, actions, rewards, total_reward = run_episode(env, neural_agent)\n",
        "    \n",
        "    # Train on this episode\n",
        "    loss = train_step(policy_net, optimizer, observations, actions, rewards)\n",
        "    \n",
        "    # Track progress\n",
        "    episode_rewards.append(total_reward)\n",
        "    \n",
        "    if episode % 20 == 0:\n",
        "        print(f\"Episode {episode}: Reward = {total_reward:.2f}, Loss = {loss:.4f}\")\n",
        "\n",
        "# Plot training progress\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(episode_rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Episode Reward')\n",
        "plt.title('Training Progress')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Render the trained agent\n",
        "render_episode(neural_agent, \"Trained Neural Agent\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Congratulations!** You've just implemented REINFORCE, one of the foundational policy gradient algorithms!\n",
        "\n",
        "This implementation includes the core concepts, but there are additional improvements you'll learn about later:\n",
        "- **Baselines**: Reducing variance in gradient estimates by subtracting a baseline from rewards\n",
        "- **Entropy bonuses**: Encouraging exploration by adding an entropy term to the loss\n",
        "- **Generalized Advantage Estimation (GAE)**: A more sophisticated way to estimate advantages\n",
        "\n",
        "These refinements can significantly improve training stability and performance!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chapter 2: Actor-Critic (A2C)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run all from part 1 :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We've successfully trained a policy network using REINFORCE! But let's think carefully about what REINFORCE is actually doing, and where it might struggle.\n",
        "\n",
        "### What's wrong with REINFORCE? (review from our session)\n",
        "\n",
        "Recall our REINFORCE gradient:\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^{N} \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot \\hat{R}_t$$\n",
        "\n",
        "This looks reasonable, but there are **two subtle problems** that can hurt learning in practice:\n",
        "\n",
        "---\n",
        "\n",
        "**Problem 1: What if all returns are positive?**\n",
        "\n",
        "Imagine an environment where all trajectories are given positive total returns (e.g you can't lose points). In REINFORCE, the gradient update *increases* the probability of actions proportionally to the return. So even bad trajectories get up-weighted!\n",
        "\n",
        "In the limit (infinite samples), this averages out correctly—worse trajectories get smaller positive updates than better ones. But in practice, with limited samples, this can significantly slow down learning or cause instability.\n",
        "\n",
        "**Partial Solution**: Subtract a **baseline** from the returns:\n",
        "$$\\nabla_\\theta J(\\theta) \\approx \\mathbb{E}\\left[\\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot (\\hat{R}_t - b)\\right]$$\n",
        "\n",
        "A common choice is $b = \\bar{R}$ (the batch mean return). This centers our updates: trajectories better than average get pushed up, worse ones get pushed down.\n",
        "\n",
        "Mathematically, subtracting any baseline $b$ that doesn't depend on actions keeps the gradient unbiased (since $\\mathbb{E}[\\nabla_\\theta \\log \\pi_\\theta(a|s)] = 0$).\n",
        "\n",
        "---\n",
        "\n",
        "**Problem 2: The Credit Assignment Problem**\n",
        "\n",
        "Look at the REINFORCE update again. Notice that **every action in a trajectory gets weighted by the same return** $J$.\n",
        "\n",
        "But what if we mostly made good decisions, with a few mistakes sprinkled in? The overall trajectory might still have a good return, so REINFORCE will *reinforce all the actions*, including the mistakes.\n",
        "\n",
        "This is the **credit assignment problem**: which specific actions actually contributed to the reward we received?\n",
        "\n",
        "REINFORCE doesn't distinguish well. A single unlucky action at $t=50$ that causes failure gets the same treatment as the good actions at $t=0,1,2,...$.\n",
        "\n",
        "---\n",
        "\n",
        "### How Actor-Critic Solves These Problems\n",
        "\n",
        "**Actor-Critic** (specifically, **Advantage Actor-Critic** or **A2C**) addresses both issues by learning a **value function** alongside the policy:\n",
        "\n",
        "1. **Against Problem 1**: Instead of using raw returns, we use **advantages** $A_t = Q(s_t, a_t) - V(s_t)$. The advantage measures \"how much better was this action compared to the average action in this state?\" This naturally centers our updates—good actions have positive advantage, bad actions have negative advantage.\n",
        "\n",
        "2. **Against Problem 2**: We estimate value at each timestep using a learned **critic** $V_\\phi(s)$. This lets us compute **per-step advantages** using the TD error:\n",
        "$$\\delta_t = r_t + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_t)$$\n",
        "\n",
        "The TD error asks: \"Given what we expected from state $s_t$, did this action lead to a better or worse outcome?\" This provides much finer-grained credit assignment—each action is evaluated based on its immediate consequence plus the value of where we ended up.\n",
        "\n",
        "---\n",
        "\n",
        "### The Actor-Critic Architecture\n",
        "\n",
        "We now have **two networks**:\n",
        "\n",
        "| Network | Name | What it learns | Output |\n",
        "|---------|------|----------------|--------|\n",
        "| $\\pi_\\theta(a\\|s)$ | **Actor** (Policy) | Which actions to take | Probability distribution over actions |\n",
        "| $V_\\phi(s)$ | **Critic** (Value) | How good is each state | Single scalar value |\n",
        "\n",
        "The critic helps the actor learn more efficiently by providing a learned baseline and enabling per-step credit assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    The Critic: estimates V(s), the expected return from state s.\n",
        "    \"\"\"\n",
        "    def __init__(self, obs_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(obs_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, 1)  # Single value output\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x).squeeze(-1)  # Shape: (batch_size,)\n",
        "\n",
        "# Quick test\n",
        "test_value_net = ValueNetwork(obs_size=4, hidden_size=32)\n",
        "test_obs = torch.randn(1, 4)\n",
        "print(f\"Value network output shape: {test_value_net(test_obs).shape}\")\n",
        "print(f\"Estimated value: {test_value_net(test_obs).item():.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Temporal Difference (TD) Error\n",
        "\n",
        "The heart of actor-critic is the **TD error** $\\delta_t$:\n",
        "\n",
        "$$\\delta_t = r_t + \\gamma V_\\phi(s_{t+1}) - V_\\phi(s_t)$$\n",
        "\n",
        "Let's break this down:\n",
        "- $V_\\phi(s_t)$: What we *expected* to get from state $s_t$\n",
        "- $r_t + \\gamma V_\\phi(s_{t+1})$: What we *actually* got (immediate reward + discounted value of next state)\n",
        "- $\\delta_t$: The **surprise**—how much better (or worse) things turned out than expected\n",
        "\n",
        "**Interpreting the TD error:**\n",
        "- $\\delta_t > 0$: \"This action led to a better outcome than I expected!\" → Increase probability of this action\n",
        "- $\\delta_t < 0$: \"This action led to a worse outcome than I expected!\" → Decrease probability of this action\n",
        "- $\\delta_t \\approx 0$: \"Things went about as expected\" → Small or no update\n",
        "\n",
        "**Why is the TD error a good advantage estimate?**\n",
        "\n",
        "Recall that the advantage is $A(s, a) = Q(s, a) - V(s)$, measuring how much better action $a$ is compared to the average. \n",
        "\n",
        "The proof is a bit more involved, but it turns out that $\\mathbb{E}[\\delta_t | s_t, a_t] = A(s_t, a_t)$ - e.g, the expected value of our defined $\\delta$ is equivalent to our advantage, which means that we're right on average.\n",
        "\n",
        "<details>\n",
        "<summary><b>Consider: Why does bootstrapping help with credit assignment?</b></summary>\n",
        "\n",
        "<br>\n",
        "\n",
        "In REINFORCE, if we take a bad action at timestep $t=50$ that eventually leads to failure at $t=100$, the return-to-go at $t=50$ will be low. But so will the returns at $t=48, 49, 51, 52$, etc.—even if those were perfectly good actions! The \"blame\" gets spread across many timesteps.\n",
        "\n",
        "With TD learning, the critic learns $V(s)$ for each state. When we take that bad action at $t=50$:\n",
        "- $V(s_{50})$ reflects the expected value *before* the bad action\n",
        "- $V(s_{51})$ reflects the expected value *after* the bad action\n",
        "- If the action was truly bad, $r_{50} + \\gamma V(s_{51}) < V(s_{50})$, giving $\\delta_{50} < 0$\n",
        "\n",
        "The TD error *isolates* the effect of each action by comparing \"before\" and \"after\" states. This is much more precise credit assignment!\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Functions for Actor-Critic\n",
        "\n",
        "We have two networks, so we need two loss functions:\n",
        "\n",
        "---\n",
        "\n",
        "**Critic Loss (Training the Value Network)**\n",
        "\n",
        "We want $V_\\phi(s)$ to accurately predict the expected return. Using TD learning, our target is $r_t + \\gamma V_\\phi(s_{t+1})$, and we minimize the squared error:\n",
        "\n",
        "$$L_{critic} = \\frac{1}{N}\\sum_t \\left(V_\\phi(s_t) - (r_t + \\gamma V_\\phi(s_{t+1}))\\right)^2 = \\frac{1}{N}\\sum_t \\delta_t^2$$\n",
        "\n",
        "This is just mean squared error (MSE) between our prediction and the TD target.\n",
        "\n",
        "**Important**: When computing the TD target $r_t + \\gamma V_\\phi(s_{t+1})$, we typically **stop gradients** through $V_\\phi(s_{t+1})$. Otherwise, the network could \"cheat\" by making both $V(s_t)$ and $V(s_{t+1})$ wrong in a consistent way.\n",
        "\n",
        "---\n",
        "\n",
        "**Actor Loss (Training the Policy Network)**\n",
        "\n",
        "Same as REINFORCE, but we use the TD error $\\delta_t$ as our advantage estimate:\n",
        "\n",
        "$$L_{actor} = -\\frac{1}{N}\\sum_t \\log \\pi_\\theta(a_t | s_t) \\cdot \\delta_t$$\n",
        "\n",
        "**Important**: Call $\\delta_t$.detach() when computing this loss. We do not want policy gradients flowing back through the value network!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary><b>Consider: Why do we stop gradients for the TD target?</b></summary>\n",
        "\n",
        "<br>\n",
        "\n",
        "Suppose we did not stop gradients. The critic loss is:\n",
        "$$L = (V_\\phi(s_t) - r_t - \\gamma V_\\phi(s_{t+1}))^2$$\n",
        "\n",
        "Gradients would flow through both $V_\\phi(s_t)$ and $V_\\phi(s_{t+1})$. The network could minimize this loss by making both predictions wrong in a way that cancels out—for example, always predicting 0 for everything!\n",
        "\n",
        "By stopping gradients through $V_\\phi(s_{t+1})$, we are saying: \"treat $r_t + \\gamma V_\\phi(s_{t+1})$ as a fixed target, and update $V_\\phi(s_t)$ to match it.\" This is the standard approach in TD learning.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise: Implement the Actor-Critic Training Step\n",
        "\n",
        "Now it's your turn! Implement a training step that:\n",
        "\n",
        "1. Computes the TD target: $y_t = r_t + \\gamma V_\\phi(s_{t+1})$ (with no gradient through $s_{t+1}$)\n",
        "2. Computes the critic loss: MSE between $V_\\phi(s_t)$ and $y_t$\n",
        "3. Updates the critic\n",
        "4. Computes TD errors (advantages): $\\delta_t = y_t - V_\\phi(s_t)$ (detached)\n",
        "5. Computes the actor loss: $-\\mathbb{E}[\\log \\pi_\\theta(a_t|s_t) \\cdot \\delta_t]$\n",
        "6. Updates the actor\n",
        "\n",
        "First, we need a modified episode collection function that returns transitions (including next states and done flags):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_episode_ac(env, agent, max_steps=1000):\n",
        "    \"\"\"\n",
        "    Run an episode and collect transitions for actor-critic training.\n",
        "    \n",
        "    Unlike our previous run_episode, we also need:\n",
        "    - next_observations: the state we transitioned TO after each action\n",
        "    - dones: whether each transition was terminal\n",
        "    \n",
        "    Returns:\n",
        "        observations, actions, rewards, next_observations, dones, total_reward\n",
        "    \"\"\"\n",
        "    observations = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    next_observations = []\n",
        "    dones = []\n",
        "    \n",
        "    observation, info = env.reset()\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        observations.append(observation)\n",
        "        \n",
        "        action = agent(observation)\n",
        "        actions.append(action)\n",
        "        \n",
        "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
        "        rewards.append(reward)\n",
        "        next_observations.append(next_observation)\n",
        "        dones.append(float(terminated))  # 1.0 if truly terminal, 0.0 otherwise\n",
        "        \n",
        "        observation = next_observation\n",
        "        \n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    \n",
        "    total_reward = sum(rewards)\n",
        "    return observations, actions, rewards, next_observations, dones, total_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now implement the training step. Remember:\n",
        "- Use `torch.no_grad()` or `.detach()` to stop gradients where needed\n",
        "- Handle terminal states: if `done=True`, then $V(s_{t+1}) = 0$ (no future rewards)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def actor_critic_train_step(policy_net, value_net, policy_optimizer, value_optimizer,\n",
        "                            observations, actions, rewards, next_observations, dones, \n",
        "                            gamma=0.99):\n",
        "    \"\"\"\n",
        "    Perform one Actor-Critic training step.\n",
        "    \n",
        "    Args:\n",
        "        policy_net: the actor network pi_theta\n",
        "        value_net: the critic network V_phi\n",
        "        policy_optimizer: optimizer for actor\n",
        "        value_optimizer: optimizer for critic\n",
        "        observations: list/array of states s_t\n",
        "        actions: list/array of actions taken a_t\n",
        "        rewards: list/array of rewards r_t\n",
        "        next_observations: list/array of next states s_{t+1}\n",
        "        dones: list/array of done flags (1.0 if terminal, 0.0 otherwise)\n",
        "        gamma: discount factor\n",
        "        \n",
        "    Returns:\n",
        "        actor_loss, critic_loss: the loss values\n",
        "    \"\"\"\n",
        "    # TODO: Convert to tensors\n",
        "    # obs_tensor = torch.tensor(np.array(observations), dtype=torch.float32)\n",
        "    # next_obs_tensor = ...\n",
        "    # action_tensor = ...\n",
        "    # reward_tensor = ...\n",
        "    # done_tensor = ...\n",
        "    \n",
        "    # ===== Step 1: Compute TD Target =====\n",
        "    # TODO: y_t = r_t + gamma * V(s_{t+1}) * (1 - done)\n",
        "    # Use torch.no_grad() to prevent gradients through the target!\n",
        "    # If terminal (done=1), next value should be 0\n",
        "    \n",
        "    # ===== Step 2: Critic Loss =====\n",
        "    # TODO: MSE between V(s_t) and the TD target\n",
        "    \n",
        "    # ===== Step 3: Update Critic =====\n",
        "    # TODO: zero_grad, backward, step\n",
        "    \n",
        "    # ===== Step 4: Compute Advantages (TD Errors) =====\n",
        "    # TODO: delta_t = y_t - V(s_t), detached so no gradients flow to critic\n",
        "    \n",
        "    # ===== Step 5: Actor Loss =====\n",
        "    # TODO: -E[log pi(a|s) * advantage]\n",
        "    \n",
        "    # ===== Step 6: Update Actor =====\n",
        "    # TODO: zero_grad, backward, step\n",
        "    \n",
        "    raise NotImplementedError()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Actor-Critic on CartPole\n",
        "\n",
        "Let us first verify our implementation works on CartPole, then we will tackle a harder environment.\n",
        "\n",
        "Note: A2C takes some time to \"spin up\" - e.g the critic takes some time to learn anything at all before it can begin to propagate a valid learning signal to the actor. This means that A2C doesn't necessarily learn faster than REINFORCE on simple environments, and will take some number of steps before its performance will improve. On harder environments, we'll see that it learns much faster than REINFORCE does.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "obs_size = env.observation_space.shape[0]  # 4\n",
        "action_size = env.action_space.n  # 2\n",
        "\n",
        "# Create networks\n",
        "policy_net = PolicyNetwork(obs_size=obs_size, hidden_size=32, action_size=action_size)\n",
        "value_net = ValueNetwork(obs_size=obs_size, hidden_size=32)\n",
        "\n",
        "# Create optimizers\n",
        "policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=1e-2)\n",
        "value_optimizer = torch.optim.Adam(value_net.parameters(), lr=1e-2)\n",
        "\n",
        "# Create agent\n",
        "ac_agent = NeuralAgent(policy_net)\n",
        "\n",
        "# Training loop\n",
        "num_episodes = 1000\n",
        "episode_rewards = []\n",
        "actor_losses = []\n",
        "critic_losses = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    # Collect episode\n",
        "    obs, actions, rewards, next_obs, dones, total_reward = run_episode_ac(env, ac_agent)\n",
        "    \n",
        "    # Train\n",
        "    actor_loss, critic_loss = actor_critic_train_step(\n",
        "        policy_net, value_net, policy_optimizer, value_optimizer,\n",
        "        obs, actions, rewards, next_obs, dones\n",
        "    )\n",
        "    \n",
        "    episode_rewards.append(total_reward)\n",
        "    actor_losses.append(actor_loss)\n",
        "    critic_losses.append(critic_loss)\n",
        "    \n",
        "    if episode % 50 == 0:\n",
        "        avg_reward = np.mean(episode_rewards[-50:]) if len(episode_rewards) >= 50 else np.mean(episode_rewards)\n",
        "        print(f\"Episode {episode}: Reward = {total_reward:.1f}, Avg(50) = {avg_reward:.1f}, \"\n",
        "              f\"Actor Loss = {actor_loss:.3f}, Critic Loss = {critic_loss:.3f}\")\n",
        "\n",
        "print(f\"\\nFinal average reward (last 50): {np.mean(episode_rewards[-50:]):.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training progress\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Episode rewards\n",
        "axes[0].plot(episode_rewards, alpha=0.6, label='Episode reward')\n",
        "if len(episode_rewards) >= 50:\n",
        "    smoothed = np.convolve(episode_rewards, np.ones(50)/50, mode='valid')\n",
        "    axes[0].plot(range(49, len(episode_rewards)), smoothed, color='red', linewidth=2, label='50-ep average')\n",
        "axes[0].set_xlabel('Episode')\n",
        "axes[0].set_ylabel('Reward')\n",
        "axes[0].set_title('Episode Rewards')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Actor loss\n",
        "axes[1].plot(actor_losses, alpha=0.6)\n",
        "axes[1].set_xlabel('Episode')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_title('Actor Loss')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Critic loss\n",
        "axes[2].plot(critic_losses, alpha=0.6)\n",
        "axes[2].set_xlabel('Episode')\n",
        "axes[2].set_ylabel('Loss')\n",
        "axes[2].set_title('Critic Loss')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Render the trained agent\n",
        "render_episode(ac_agent, \"Actor-Critic Agent (CartPole)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge: LunarLander-v3\n",
        "\n",
        "CartPole is a great testbed, but we are saturating on it—both REINFORCE and Actor-Critic solve it quickly, making it hard to see the difference! Let us try a harder environment where Actor-Critic's advantages really shine: **LunarLander-v3**.\n",
        "\n",
        "**About LunarLander:**\n",
        "- **State space**: 8-dimensional (position, velocity, angle, angular velocity, leg contact)\n",
        "- **Action space**: 4 discrete actions (do nothing, fire left engine, fire main engine, fire right engine)\n",
        "- **Reward**: +100 to +140 for landing, -100 for crash, small rewards for moving toward landing pad, penalties for using fuel\n",
        "- **Goal**: Land safely on the landing pad (between the flags)\n",
        "\n",
        "This environment is significantly harder:\n",
        "- Larger state and action spaces\n",
        "- Sparse rewards (most signal comes at the end)\n",
        "- Need to learn precise control\n",
        "\n",
        "<details>\n",
        "<summary><b>Consider: Why might Actor-Critic outperform REINFORCE on LunarLander?</b></summary>\n",
        "\n",
        "<br>\n",
        "\n",
        "LunarLander episodes can be long, and the final outcome (crash vs. safe landing) dominates the return. With REINFORCE, if we crash at the very end, *all* actions in the trajectory get blamed equally—even the good ones at the start!\n",
        "\n",
        "With Actor-Critic:\n",
        "1. The critic learns the value of different states, providing per-step feedback\n",
        "2. Good early actions that put us in high-value states get positive TD errors\n",
        "3. The fatal action that caused the crash gets a very negative TD error\n",
        "4. Credit is assigned more precisely to the actions that actually mattered\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install box2d for LunarLander (if needed)\n",
        "%pip install \"gymnasium[box2d]\" -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LunarLander environment\n",
        "ll_env = gym.make(\"LunarLander-v3\")\n",
        "ll_obs_size = ll_env.observation_space.shape[0]  # 8\n",
        "ll_action_size = ll_env.action_space.n  # 4\n",
        "\n",
        "print(f\"LunarLander observation space: {ll_env.observation_space}\")\n",
        "print(f\"LunarLander action space: {ll_env.action_space}\")\n",
        "print(f\"Obs size: {ll_obs_size}, Action size: {ll_action_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create networks for LunarLander (larger hidden size for harder task)\n",
        "ll_policy_net = PolicyNetwork(obs_size=ll_obs_size, hidden_size=128, action_size=ll_action_size)\n",
        "ll_value_net = ValueNetwork(obs_size=ll_obs_size, hidden_size=128)\n",
        "\n",
        "# Create optimizers\n",
        "ll_policy_optimizer = torch.optim.Adam(ll_policy_net.parameters(), lr=1e-3)\n",
        "ll_value_optimizer = torch.optim.Adam(ll_value_net.parameters(), lr=1e-3)\n",
        "\n",
        "# Create agent\n",
        "ll_agent = NeuralAgent(ll_policy_net)\n",
        "\n",
        "# Training loop\n",
        "num_episodes = 2000 # may take a while\n",
        "ll_episode_rewards = []\n",
        "ll_actor_losses = []\n",
        "ll_critic_losses = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    # Collect episode\n",
        "    obs, actions, rewards, next_obs, dones, total_reward = run_episode_ac(ll_env, ll_agent)\n",
        "    \n",
        "    # Train\n",
        "    actor_loss, critic_loss = actor_critic_train_step(\n",
        "        ll_policy_net, ll_value_net, ll_policy_optimizer, ll_value_optimizer,\n",
        "        obs, actions, rewards, next_obs, dones, gamma=0.95\n",
        "    )\n",
        "    \n",
        "    ll_episode_rewards.append(total_reward)\n",
        "    ll_actor_losses.append(actor_loss)\n",
        "    ll_critic_losses.append(critic_loss)\n",
        "    \n",
        "    if episode % 100 == 0:\n",
        "        avg_reward = np.mean(ll_episode_rewards[-100:]) if len(ll_episode_rewards) >= 100 else np.mean(ll_episode_rewards)\n",
        "        print(f\"Episode {episode}: Reward = {total_reward:.1f}, Avg(100) = {avg_reward:.1f}, \"\n",
        "              f\"Actor Loss = {actor_loss:.3f}, Critic Loss = {critic_loss:.3f}\")\n",
        "\n",
        "print(f\"\\nFinal average reward (last 100): {np.mean(ll_episode_rewards[-100:]):.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot LunarLander training progress\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Episode rewards\n",
        "axes[0].plot(ll_episode_rewards, alpha=0.4, label='Episode reward')\n",
        "if len(ll_episode_rewards) >= 100:\n",
        "    smoothed = np.convolve(ll_episode_rewards, np.ones(100)/100, mode='valid')\n",
        "    axes[0].plot(range(99, len(ll_episode_rewards)), smoothed, color='red', linewidth=2, label='100-ep average')\n",
        "axes[0].axhline(y=200, color='green', linestyle='--', alpha=0.5, label='Solved threshold')\n",
        "axes[0].set_xlabel('Episode')\n",
        "axes[0].set_ylabel('Reward')\n",
        "axes[0].set_title('LunarLander Episode Rewards')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Actor loss\n",
        "axes[1].plot(ll_actor_losses, alpha=0.6)\n",
        "axes[1].set_xlabel('Episode')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_title('Actor Loss')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Critic loss\n",
        "axes[2].plot(ll_critic_losses, alpha=0.6)\n",
        "axes[2].set_xlabel('Episode')\n",
        "axes[2].set_ylabel('Loss')\n",
        "axes[2].set_title('Critic Loss')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def render_lunar_lander(agent, agent_name=\"Agent\"):\n",
        "    \"\"\"Render a LunarLander episode.\"\"\"\n",
        "    env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "    \n",
        "    frames = []\n",
        "    observation, info = env.reset()\n",
        "    frames.append(env.render())\n",
        "    \n",
        "    total_reward = 0\n",
        "    for step in range(1000):\n",
        "        action = agent(observation)\n",
        "        observation, reward, terminated, truncated, info = env.step(action)\n",
        "        frames.append(env.render())\n",
        "        total_reward += reward\n",
        "        \n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    \n",
        "    env.close()\n",
        "    \n",
        "    # Create animation\n",
        "    fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    ax.set_title(f\"{agent_name} (Total Reward: {total_reward:.1f})\")\n",
        "    ax.axis('off')\n",
        "    \n",
        "    img = ax.imshow(frames[0])\n",
        "    \n",
        "    def animate(i):\n",
        "        img.set_array(frames[i])\n",
        "        return [img]\n",
        "    \n",
        "    anim = animation.FuncAnimation(fig, animate, frames=len(frames), interval=50, blit=True)\n",
        "    plt.close()\n",
        "    \n",
        "    return HTML(anim.to_jshtml())\n",
        "\n",
        "render_lunar_lander(ll_agent, \"Actor-Critic Agent (LunarLander)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ll_env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### To remember: REINFORCE vs Actor-Critic\n",
        "\n",
        "| Aspect | REINFORCE | Actor-Critic (A2C) |\n",
        "|--------|-----------|-------------------|\n",
        "| **Networks** | Policy only | Policy (actor) + Value (critic) |\n",
        "| **Weight for updates** | Reward-to-go $\\hat{R}_t$ | TD error $\\delta_t$ (advantage) |\n",
        "| **Baseline** | None (or batch mean) | Learned value function $V(s)$ |\n",
        "| **Credit assignment** | All actions weighted by trajectory return | Per-step: each action judged by immediate outcome |\n",
        "| **Variance** | High | Lower (bootstrapping from learned $V$) |\n",
        "| **Bias** | Unbiased | Slightly biased (imperfect $V$) |\n",
        "\n",
        "**Key takeaways:**\n",
        "\n",
        "1. **Actor-Critic reduces variance** by using a learned baseline $V(s)$ instead of raw returns\n",
        "\n",
        "2. **TD errors provide better credit assignment** by comparing \"before\" and \"after\" states for each action\n",
        "\n",
        "3. **The critic learns to predict value**, which helps both as a baseline and for computing per-step advantages\n",
        "\n",
        "4. **The tradeoff**: We introduce some bias (our $V$ is not perfect), but the variance reduction typically leads to faster, more stable learning\n",
        "\n",
        "**What is next?** There are many ways to extend Actor-Critic:\n",
        "- **GAE (Generalized Advantage Estimation)**: Interpolate between TD(0) and full returns\n",
        "- **Entropy bonus**: Encourage exploration by adding policy entropy to the loss\n",
        "- **A3C/A2C parallelism**: Run multiple environments in parallel for faster data collection\n",
        "- **PPO (Proximal Policy Optimization)**: Constrain policy updates for stability\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
